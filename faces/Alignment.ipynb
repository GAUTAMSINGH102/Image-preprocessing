{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6db2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc272520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths for new train and validation folders\n",
    "new_train_folder = \"new_train\"\n",
    "new_validation_folder = \"new_validation\"\n",
    "\n",
    "# Create new train and validation folders if they don't exist\n",
    "os.makedirs(new_train_folder, exist_ok=True)\n",
    "os.makedirs(new_validation_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675f5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector and landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223e5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face(image, landmarks):\n",
    "    # Define desired landmarks for alignment\n",
    "    desired_landmarks = np.array([\n",
    "        (0.315, 0.393),  # Left eye corner\n",
    "        (0.685, 0.393),  # Right eye corner\n",
    "        (0.500, 0.618),  # Nose tip\n",
    "        (0.370, 0.800),  # Left mouth corner\n",
    "        (0.630, 0.800)   # Right mouth corner\n",
    "    ])\n",
    "    \n",
    "    # Convert landmarks to NumPy array and reshape for compatibility\n",
    "    landmarks = np.array(landmarks).reshape(-1, 2)\n",
    "    \n",
    "    # Calculate transformation matrix using desired and detected landmarks\n",
    "    aligned_image_size = (256, 256)\n",
    "    transformation_matrix = cv2.estimateAffinePartial2D(\n",
    "        landmarks, desired_landmarks * aligned_image_size[0])[0]\n",
    "    \n",
    "    # Apply transformation matrix to align the face\n",
    "    aligned_image = cv2.warpAffine(\n",
    "        image, transformation_matrix, aligned_image_size,\n",
    "        flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "    \n",
    "    return aligned_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f30817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the training set\n",
    "train_folder = \"faceEmotionDetection/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a39137",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\ptsetreg.cpp:1108: error: (-215:Assertion failed) count >= 0 && to.checkVector(2) == count in function 'cv::estimateAffinePartial2D'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m landmarks \u001b[38;5;241m=\u001b[39m predictor(gray, face)\n\u001b[0;32m     17\u001b[0m landmarks \u001b[38;5;241m=\u001b[39m [(landmark\u001b[38;5;241m.\u001b[39mx, landmark\u001b[38;5;241m.\u001b[39my) \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m landmarks\u001b[38;5;241m.\u001b[39mparts()]\n\u001b[1;32m---> 18\u001b[0m aligned_face \u001b[38;5;241m=\u001b[39m \u001b[43malign_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Perform additional preprocessing (e.g., resizing, normalization, etc.) as needed\u001b[39;00m\n\u001b[0;32m     22\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(image_file)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36malign_face\u001b[1;34m(image, landmarks)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate transformation matrix using desired and detected landmarks\u001b[39;00m\n\u001b[0;32m     15\u001b[0m aligned_image_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m transformation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimateAffinePartial2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_landmarks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maligned_image_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Apply transformation matrix to align the face\u001b[39;00m\n\u001b[0;32m     20\u001b[0m aligned_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpAffine(\n\u001b[0;32m     21\u001b[0m     image, transformation_matrix, aligned_image_size,\n\u001b[0;32m     22\u001b[0m     flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_LINEAR, borderMode\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mBORDER_REFLECT_101)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\ptsetreg.cpp:1108: error: (-215:Assertion failed) count >= 0 && to.checkVector(2) == count in function 'cv::estimateAffinePartial2D'\n"
     ]
    }
   ],
   "source": [
    "for class_folder in os.listdir(train_folder):\n",
    "    class_path = os.path.join(train_folder, class_folder)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    \n",
    "    new_class_folder = os.path.join(new_train_folder, class_folder)\n",
    "    os.makedirs(new_class_folder, exist_ok=True)\n",
    "    \n",
    "    image_files = glob.glob(os.path.join(class_path, \"*.jpg\"))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faces = detector(gray)\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "            landmarks = [(landmark.x, landmark.y) for landmark in landmarks.parts()]\n",
    "            aligned_face = align_face(image, landmarks)\n",
    "            \n",
    "            # Perform additional preprocessing (e.g., resizing, normalization, etc.) as needed\n",
    "            \n",
    "            filename = os.path.basename(image_file)\n",
    "            new_filepath = os.path.join(new_class_folder, filename)\n",
    "            \n",
    "            cv2.imwrite(new_filepath, aligned_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e37eb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the validation set\n",
    "validation_folder = \"/content/drive/MyDrive/Kathak/faceEmotionDetection/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56587c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_folder in os.listdir(validation_folder):\n",
    "    class_path = os.path.join(validation_folder, class_folder)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    \n",
    "    new_class_folder = os.path.join(new_validation_folder, class_folder)\n",
    "    os.makedirs(new_class_folder, exist_ok=True)\n",
    "    \n",
    "    image_files = glob.glob(os.path.join(class_path, \"*.jpg\"))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faces = detector(gray)\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "            landmarks = [(landmark.x, landmark.y) for landmark in landmarks.parts()]\n",
    "            aligned_face = align_face(image, landmarks)\n",
    "\n",
    "            # Perform additional preprocessing (e.g., resizing, normalization, etc.) as needed\n",
    "\n",
    "            filename = os.path.basename(image_file)\n",
    "            new_filepath = os.path.join(new_class_folder, filename)\n",
    "\n",
    "            cv2.imwrite(new_filepath, aligned_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad789d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
